from __future__ import annotations
import os, re, hashlib, numpy as np
from typing import List, Any, Dict

# === LangChain core ===
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA, ConversationChain
from langchain.agents import initialize_agent, AgentType
from langchain.memory import ConversationBufferMemory
from langchain.prompts import ChatPromptTemplate
from langchain.tools import Tool
from langchain.document_loaders import PyPDFLoader

# === ç¯å¢ƒå˜é‡ ===
from dotenv import load_dotenv
load_dotenv()
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
EMBEDDINGS_BACKEND = os.getenv('EMBEDDINGS_BACKEND', 'OPENAI').upper()
VECTORSTORE_BACKEND = os.getenv('VECTORSTORE_BACKEND', 'CHROMA').upper()
PDF_PATH = 'backend/Track_B_Tenancy_Agreement.pdf'

print(f'ğŸ” OPENAI_API_KEY set: {bool(OPENAI_API_KEY)}')
print(f'ğŸ§  EMBEDDINGS_BACKEND = {EMBEDDINGS_BACKEND}')
print(f'ğŸ’¾ VECTORSTORE_BACKEND = {VECTORSTORE_BACKEND}')
print(f'ğŸ“„ PDF_PATH = {PDF_PATH}')

# === åŠ è½½ PDF ===
try:
    loader = PyPDFLoader(PDF_PATH)
    docs = loader.load()
    print(f'ğŸ“„ æˆåŠŸåŠ è½½ {len(docs)} é¡µ')
except Exception as e:
    print('â—æ— æ³•åŠ è½½ PDF:', e)
    docs = []

# === æ„å»º Embeddings ===
if EMBEDDINGS_BACKEND == 'OPENAI':
    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
print('âœ… Embeddings ready')

# === æ„å»ºå‘é‡åº“ ===
if docs:
    vectorstore = Chroma.from_documents(docs, embedding=embeddings)
    print('âœ… Vector store ready (Chroma)')
else:
    vectorstore = None
    print('âš ï¸ No docs loaded.')

# === å®šä¹‰ Prompt ===
contract_prompt = ChatPromptTemplate.from_messages([
    ("system",
     "You are a professional Singapore tenancy-law assistant. "
     "Answer only based on the provided contract context."),
    ("human",
     "Context:\n{context}\n\nQuestion:\n{question}\n\n"  # âœ… æ³¨æ„æ˜¯ question
     "Answer format:\n"
     "1. Short answer\n"
     "2. Clause reference\n"
     "3. Source snippet\n\n"
     "If not found, reply: 'The provided contract does not contain this information.'")
])
print("ğŸ§¾ Contract prompt ready.")

general_prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a friendly and knowledgeable Singapore tenancy assistant."),
    ("human", "{user_query}")
])
print("ğŸ’¬ General prompt ready.")

# === åˆå§‹åŒ– LLM ===
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.2, api_key=OPENAI_API_KEY)
print('ğŸ§  LLM ready.')

# === æ„å»º RetrievalQA ===
if vectorstore:
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(),
        chain_type_kwargs={"prompt": contract_prompt}
    )
    print('âœ… RetrievalQA chain ready with contract prompt.')
else:
    qa_chain = None

# === å·¥å…·å‡½æ•° ===
def calculate_rent_tool(query: str) -> str:
    nums = [int(x) for x in re.findall(r"\d+", query)]
    if len(nums) >= 2:
        monthly, months = nums[0], nums[1]
        total = monthly * months
        return f"ğŸ’° Estimated total rent for {months} months at ${monthly}/mo: **${total}**."
    return "Please provide both the monthly rent and number of months (e.g., '$2500 for 15 months')."

calculate_rent = Tool.from_function(
    func=calculate_rent_tool,
    name="calculate_rent",
    description="Calculate total rent given monthly rent and number of months from natural language."
)
print('ğŸ§° Tool ready.')

# === Memory & Agent ===
memory = ConversationBufferMemory()
agent = initialize_agent(
    tools=[calculate_rent],
    llm=llm,
    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,
    verbose=False
)
print('ğŸ§© Agent ready.')

# === ä¸»ç±» ===
class TenantChatbot:
    """å¤šæ„å›¾ç§ŸèµåˆåŒ Chatbotï¼šåˆåŒé—®ç­” / è®¡ç®— / ä¸€èˆ¬å¯¹è¯"""
    def __init__(self, docs, vectorstore, llm, memory, qa_chain, agent):
        self.docs = docs
        self.vectorstore = vectorstore
        self.llm = llm
        self.memory = memory
        self.qa_chain = qa_chain
        self.agent = agent
        self.conversation = ConversationChain(llm=self.llm, memory=self.memory)

        self.contract_keywords = [
            'clause', 'tenant', 'landlord', 'terminate', 'repair', 'deposit',
            'renewal', 'maintenance', 'aircon', 'breach', 'notice', 'early termination'
        ]
        self.calc_keywords = ['calculate', 'rent', 'payment', 'fee', 'total']

    def process_query(self, query: str) -> str:
        q = query.lower()

        # ğŸ§¾ 1ï¸âƒ£ åˆåŒç›¸å…³é—®é¢˜ â†’ ä½¿ç”¨ RAG
        if any(k in q for k in self.contract_keywords):
            if not self.qa_chain:
                return 'RAG æœªå°±ç»ªï¼šç¼ºå°‘å‘é‡åº“æˆ– LLM é…ç½®ã€‚'
            result = self.qa_chain.invoke({"query": query})  # âœ… è¿™é‡Œä»ç„¶ä¼  queryï¼ˆRetrievalQA å†…éƒ¨ä¼šè½¬æˆ questionï¼‰
            return result["result"]

        # ğŸ’° 2ï¸âƒ£ è®¡ç®—é—®é¢˜
        if any(k in q for k in self.calc_keywords):
            try:
                return self.agent.run(query)
            except Exception as e:
                return f'Agent æ‰§è¡Œå¤±è´¥: {e}'

        # ğŸ’¬ 3ï¸âƒ£ ä¸€èˆ¬èŠå¤©
        try:
            formatted = general_prompt.format_messages(user_query=query)
            response = self.llm.invoke(formatted)
            return response.content
        except Exception as e:
            return f'ä¼šè¯å¤±è´¥: {e}'

print('ğŸ—ï¸ TenantChatbot ready.')

# === åˆ›å»ºå®ä¾‹ ===
chatbot = TenantChatbot(
    docs=docs,
    vectorstore=vectorstore,
    llm=llm,
    memory=memory,
    qa_chain=qa_chain,
    agent=agent
)

# === æµ‹è¯• ===
print("\nğŸ§ª Test queries:")
for q in [
    "Who is responsible for aircon maintenance?",
    "Calculate total rent if monthly rent is $2500 for 12 months.",
    "Hi, can you explain what a tenancy agreement means?"
]:
    print(f"\nğŸ‘¤ Q: {q}")
    print(f"ğŸ¤– A: {chatbot.process_query(q)}")
