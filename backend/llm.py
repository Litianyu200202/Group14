from __future__ import annotations

import os
from typing import List, Any, Dict

# LangChain core
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA, ConversationChain
from langchain.agents import initialize_agent, AgentType
from langchain.memory import ConversationBufferMemory
from langchain.prompts import ChatPromptTemplate
from langchain.tools import Tool
from langchain.document_loaders import PyPDFLoader

# Utilities
import re
import hashlib
import numpy as np

print('âœ… Libraries imported.')


# === API Key ===
from dotenv import load_dotenv
load_dotenv()  # ä» .env æ–‡ä»¶åŠ è½½ç¯å¢ƒå˜é‡
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')

# === Backend Switches ===
EMBEDDINGS_BACKEND = os.getenv('EMBEDDINGS_BACKEND', 'OPENAI').upper()   # 'OPENAI' or 'LOCAL'
VECTORSTORE_BACKEND = os.getenv('VECTORSTORE_BACKEND', 'CHROMA').upper() # 'CHROMA' (default)

PDF_PATH = 'backend/Track_B_Tenancy_Agreement.pdf'  # è¯·å°†åˆåŒæ”¾åœ¨åŒç›®å½• / Put the PDF in the same folder

print(f'ğŸ” OPENAI_API_KEY set: {bool(OPENAI_API_KEY)}')
print(f'ğŸ§  EMBEDDINGS_BACKEND = {EMBEDDINGS_BACKEND}')
print(f'ğŸ’¾ VECTORSTORE_BACKEND = {VECTORSTORE_BACKEND}')
print(f'ğŸ“„ PDF_PATH = {PDF_PATH}')


# åŸæœ‰åŠ è½½é€»è¾‘ï¼ˆä¿æŒä¸åŠ¨ / Kept as-isï¼‰
try:
    loader = PyPDFLoader(PDF_PATH)
    docs = loader.load()
    print(f'ğŸ“„ æˆåŠŸåŠ è½½ {len(docs)} é¡µ / Loaded {len(docs)} pages.')
except Exception as e:
    print('â—æ— æ³•åŠ è½½PDFï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ã€‚/ Failed to load PDF.')
    print('Error:', e)
    docs = []



# æ„å»ºåµŒå…¥å™¨
if EMBEDDINGS_BACKEND == 'OPENAI':
    if not OPENAI_API_KEY:
        raise RuntimeError('OPENAI_API_KEY æœªè®¾ç½®ï¼Œä½† EMBEDDINGS_BACKEND=OPENAIã€‚è¯·è®¾ç½®ç¯å¢ƒå˜é‡æˆ–åˆ‡æ¢åˆ° LOCALã€‚')
    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)


print('âœ… Embeddings ready:', type(embeddings).__name__)

# æ„å»ºå‘é‡æ•°æ®åº“ï¼ˆé»˜è®¤ Chromaï¼‰/ Build vector store
if not docs:
    print('âš ï¸ æ²¡æœ‰æ–‡æ¡£å¯ç”¨äºæ„å»ºå‘é‡åº“ / No docs for vector store.')
    vectorstore = None
else:
    # Chroma in-memory; you can set persist_directory for persistence
    vectorstore = Chroma.from_documents(docs, embedding=embeddings)
    print('âœ… Vector store ready: Chroma (memory)')

contract_prompt = ChatPromptTemplate.from_messages([
    ("system",
     "You are a professional Singapore tenancy-law assistant. "
     "Use the given contract context to answer clearly and cite the relevant clause."),
    ("human",
     "Context:\n{context}\n\n"
     "Question:\n{user_query}\n\n"
     "Answer format:\n"
     "1. Short answer\n"
     "2. Clause reference\n"
     "3. Source snippet")
])
print("ğŸ§¾ Template: Contract-based Q&A Assistant Created")


# åˆå§‹åŒ– LLM
if OPENAI_API_KEY:
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.2, api_key=OPENAI_API_KEY)
else:
    # ä»åˆ›å»ºå¯¹è±¡ä»¥ä¿æŒæ¥å£ä¸€è‡´ï¼ˆå¦‚æœ SDK å¼ºæ ¡éªŒï¼Œä¼šæŠ›é”™ï¼›å»ºè®®è®¾ç½® Keyï¼‰
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.2, api_key="")
    print("âš ï¸ æœªè®¾ç½® OPENAI_API_KEYï¼Œåç»­çœŸå®é—®ç­”å°†æ— æ³•å·¥ä½œã€‚Set OPENAI_API_KEY to use real LLM.")

# åˆ›å»º QA é“¾
if vectorstore is not None:
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever()
    )
    print('âœ… RetrievalQA chain is ready.')
else:
    qa_chain = None
    print('âš ï¸ QA chain skipped due to missing vectorstore.')

sample_query = "Who is responsible for aircon maintenance?"
sample_context = (
    "Clause 2(j): The tenant shall be responsible for minor repairs not exceeding S$200. "
    "Air-conditioning servicing to be carried out once every three months by the tenant."
)
formatted_prompt = contract_prompt.format_messages(
    context=sample_context,
    user_query=sample_query
)
print('ğŸ”§ Formatted messages preview:')
for m in formatted_prompt:
    print(f'[{m.type}] {m.content[:120]}...')

def ensure_vector_store():
    """
    ç¡®ä¿å‘é‡æ•°æ®åº“å­˜åœ¨ã€‚å¦‚æœä¸å­˜åœ¨åˆ™é‡æ–°åˆ›å»ºã€‚
    """
    from langchain_community.vectorstores import Chroma
    from langchain_openai import OpenAIEmbeddings
    import os

    persist_dir = "backend/vector_store"

    if not os.path.exists(persist_dir) or not os.listdir(persist_dir):
        print("âš™ï¸ æœªæ£€æµ‹åˆ°å‘é‡åº“ï¼Œæ­£åœ¨é‡æ–°åˆ›å»º...")
        embeddings = OpenAIEmbeddings()
        vectordb = Chroma(persist_directory=persist_dir, embedding_function=embeddings)
        print("âœ… å‘é‡åº“åˆ›å»ºå®Œæˆã€‚")
    else:
        print("âœ… å·²æ£€æµ‹åˆ°ç°æœ‰å‘é‡åº“ï¼Œè·³è¿‡é‡å»ºã€‚")

    return Chroma(persist_directory=persist_dir, embedding_function=OpenAIEmbeddings())

def debug_query(self, query: str):
    q = query.lower()
    contract_match = any(k in q for k in self.contract_keywords)
    calc_match = any(k in q for k in self.calc_keywords)
    print(f"Query: {query}")
    print(f"Contract match: {contract_match}, Calc match: {calc_match}")
    print(f"Vectorstore ready: {self.qa_chain is not None}")
    return self.process_query(query)


def calculate_rent_tool(query: str) -> str:
    """ä»è‡ªç„¶è¯­è¨€ä¸­æå– (monthly_rent, months) å¹¶ä¼°ç®—æ€»ç§Ÿé‡‘ã€‚
    Extract (monthly_rent, months) from text and compute total rent.
    ç¤ºä¾‹: "Calculate total rent if monthly rent is $2500 for 15 months."
    """
    nums = [int(x) for x in re.findall(r"\d+", query)]
    monthly = months = None
    if len(nums) >= 2:
        # æœ´ç´ å‡è®¾ï¼šç¬¬ä¸€ä¸ªæ•°=æœˆç§Ÿï¼Œç¬¬äºŒä¸ªæ•°=æœˆæ•° / naive assumption
        monthly, months = nums[0], nums[1]
        total = monthly * months
        return f"ğŸ’° Estimated total rent for {months} months at ${monthly}/mo: **${total}**."
    return "Please provide both the monthly rent and the number of months (e.g., '$2500 for 15 months')."

calculate_rent = Tool.from_function(
    func=calculate_rent_tool,
    name="calculate_rent",
    description="Calculate total rent given monthly rent and number of months from natural language."
)
print('ğŸ§° Tool ready: calculate_rent')


memory = ConversationBufferMemory()

agent = initialize_agent(
    tools=[calculate_rent],
    llm=llm,
    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,
    verbose=False
)

print('ğŸ§  Memory ready. ğŸ¤– Agent ready.')

class TenantChatbot:
    """ç»Ÿä¸€å¤šæ„å›¾å…¥å£çš„ç§ŸèµåˆåŒ Chatbot / Unified multi-intent Tenant Chatbot."""
    def __init__(self, docs, vectorstore, llm, memory, qa_chain, agent):
        self.docs = docs
        self.vectorstore = vectorstore
        self.llm = llm
        self.memory = memory
        self.qa_chain = qa_chain
        self.agent = agent
        self.conversation = ConversationChain(llm=self.llm, memory=self.memory)

        # å¯æŒ‰éœ€æ‰©å±•çš„å…³é”®å­—ï¼ˆå¯è¿ç§»åˆ°é…ç½® / You can externalize these intent keywordsï¼‰
        self.contract_keywords = [
            'clause', 'tenant', 'landlord', 'terminate', 'repair', 'deposit',
            'renewal', 'maintenance', 'aircon', 'breach', 'notice', 'early termination'
        ]
        self.calc_keywords = ['calculate', 'rent', 'payment', 'fee', 'total']

    def process_query(self, query: str) -> str:
        q = query.lower()

        # 1) åˆåŒæ¡æ¬¾ç±»é—®é¢˜ â†’ ä½¿ç”¨ RAGï¼ˆå‘é‡æ£€ç´¢ + LLMï¼‰
        if any(k in q for k in self.contract_keywords):
            if not self.qa_chain:
                return 'RAG æœªå°±ç»ªï¼šç¼ºå°‘å‘é‡åº“æˆ– LLM é…ç½®ã€‚/ RAG is not ready (missing vector store or LLM).'
            return self.qa_chain.run(query)

        # 2) è®¡ç®—/å·¥å…·ç±»é—®é¢˜ â†’ äº¤ç»™ Agent ä¸å·¥å…·
        if any(k in q for k in self.calc_keywords):
            try:
                return self.agent.run(query)
            except Exception as e:
                return f'Agent æ‰§è¡Œå¤±è´¥ / Agent failed: {e}'

        # 3) ä¸€èˆ¬æ€§äº¤æµæˆ–æŒ‡å¯¼ â†’ èµ°è®°å¿†ä¼šè¯
        try:
            return self.conversation.invoke({"input": query})["response"]
        except Exception as e:
            return f'ä¼šè¯å¤±è´¥ / Conversation failed: {e}'

print('ğŸ—ï¸ TenantChatbot class ready.')
chatbot = TenantChatbot(
    docs=docs,
    vectorstore=vectorstore,
    llm=llm,
    memory=memory,
    qa_chain=qa_chain,
    agent=agent
)
