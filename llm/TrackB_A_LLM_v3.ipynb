{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d09266ae",
   "metadata": {},
   "source": [
    "\n",
    "# Track B — A(LLM) \n",
    "**Scope**: A-role only (LLM, Prompts, RAG, Intent).  \n",
    "**Note**: This notebook uses an in-memory vector store for demo. Replace with pgvector for production.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aff8ad",
   "metadata": {},
   "source": [
    "\n",
    "## 0️⃣ Setup\n",
    "- Set `OPENAI_API_KEY` (env var or fill in the cell).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397a979d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal imports and client bootstrap (safe, no hard failure)\n",
    "import os, json, textwrap\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "\n",
    "# Optional imports: guarded to avoid notebook import-time crashes\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "    openai_available = True\n",
    "except Exception:\n",
    "    OpenAI = None\n",
    "    openai_available = False\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "if openai_available and OPENAI_API_KEY:\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "else:\n",
    "    client = None\n",
    "\n",
    "PDF_PATH = \"Track_B_Tenancy_Agreement.pdf\"\n",
    "print(\"OpenAI available:\", openai_available, \"| API key set:\", bool(OPENAI_API_KEY))\n",
    "print(\"PDF path:\", PDF_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe9cdba",
   "metadata": {},
   "source": [
    "\n",
    "## 1️⃣ Prompt Templates\n",
    "合同问答（带引用、JSON输出）与意图识别（三分类）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94952449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧩 Fused Prompt Definitions \n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are an expert assistant for tenancy contracts. \"\n",
    "    \"Use only the information provided in the contract context to answer. \"\n",
    "    \"Cite clause numbers (e.g., 'Clause 2(b)') when referencing them. \"\n",
    "    \"Never fabricate information that is not explicitly supported by the contract. \"\n",
    "    \"If uncertain, say 'Not sure, please check with landlord.' \"\n",
    "    \"Always respond concisely in English.\"\n",
    ")\n",
    "\n",
    "CONTRACT_QA_PROMPT = \"\"\"\n",
    "You are a tenancy contract assistant. \n",
    "Use ONLY the contract excerpts below to answer the user's question.\n",
    "\n",
    "--- CONTRACT CONTEXT START ---\n",
    "{context}\n",
    "--- CONTRACT CONTEXT END ---\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Guidelines:\n",
    "- Answer based strictly on the given clauses.\n",
    "- If the topic is not covered, say: \"Not sure, please check with landlord.\"\n",
    "- Cite clause numbers (e.g., 'Clause 2(b)') where applicable.\n",
    "- Do NOT guess or fabricate.\n",
    "- Provide your final answer in JSON format:\n",
    "{{\n",
    "  \"answer\": \"...\",\n",
    "  \"citations\": [{{\"clause\":\"...\", \"pages\":[...]}}],\n",
    "  \"confidence\": \"high|medium|low\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "print(\"✅ Fused prompt templates loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37cab76",
   "metadata": {},
   "source": [
    "\n",
    "## 2️⃣ Intent Classification\n",
    "温度 0，输出严格为三选一。若未配置 OpenAI，将提示未就绪。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8597ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_intent(user_input: str) -> str:\n",
    "    if client is None:\n",
    "        return \"(OpenAI not configured)\"\n",
    "    prompt = INTENT_PROMPT.format(user_input=user_input)\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\":\"user\",\"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    label = resp.choices[0].message.content.strip().lower()\n",
    "    if label not in {\"contract_qa\",\"repair_request\",\"status_check\"}:\n",
    "        label = \"contract_qa\"\n",
    "    return label\n",
    "\n",
    "print(\"Intent function ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7c0737",
   "metadata": {},
   "source": [
    "\n",
    "## 3️⃣ Contract Ingestion (PDF → Chunks → Embeddings)\n",
    "以 token 数分块并向量化。此处使用内存 `VECTOR_DB` 演示；联调时替换为 **B组** 的数据库接口。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64f2409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer (guarded import)\n",
    "try:\n",
    "    import tiktoken\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    token_ok = True\n",
    "except Exception:\n",
    "    tokenizer = None\n",
    "    token_ok = False\n",
    "\n",
    "VECTOR_DB: List[Dict[str, Any]] = []\n",
    "\n",
    "def split_text_by_tokens(text: str, max_tokens=900, overlap=150):\n",
    "    if not token_ok or tokenizer is None:\n",
    "        # naive fallback by characters if tiktoken not installed\n",
    "        text = (text or \"\").strip()\n",
    "        if not text:\n",
    "            return []\n",
    "        chunks = []\n",
    "        step = 2500  # rough char window\n",
    "        o = 400\n",
    "        for i in range(0, len(text), step - o):\n",
    "            chunks.append(text[i:i+step])\n",
    "        return chunks\n",
    "\n",
    "    text = (text or \"\").strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    toks = tokenizer.encode(text)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(toks):\n",
    "        end = min(start + max_tokens, len(toks))\n",
    "        chunk = tokenizer.decode(toks[start:end])\n",
    "        chunks.append(chunk)\n",
    "        if end == len(toks): break\n",
    "        start = max(0, end - overlap)\n",
    "    return chunks\n",
    "\n",
    "def insert_document_chunk(doc_id: str, page: int, content: str, embedding: List[float]):\n",
    "    VECTOR_DB.append({\n",
    "        \"doc_id\": doc_id,\n",
    "        \"page\": page,\n",
    "        \"content\": content,\n",
    "        \"embedding\": np.array(embedding, dtype=np.float32)\n",
    "    })\n",
    "\n",
    "def process_contract_pdf(pdf_path: str, doc_id: str = \"TA-EXAMPLE\"):\n",
    "    if client is None:\n",
    "        print(\"⚠️ OpenAI 未配置，跳过向量化。\")\n",
    "        return\n",
    "    try:\n",
    "        import pdfplumber\n",
    "    except Exception:\n",
    "        print(\"⚠️ 未安装 pdfplumber，跳过解析。\")\n",
    "        return\n",
    "    total = 0\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for i, page in enumerate(pdf.pages, start=1):\n",
    "            text = page.extract_text() or \"\"\n",
    "            for chunk in split_text_by_tokens(text):\n",
    "                emb = client.embeddings.create(\n",
    "                    model=\"text-embedding-3-large\",\n",
    "                    input=chunk\n",
    "                ).data[0].embedding\n",
    "                insert_document_chunk(doc_id, i, chunk, emb)\n",
    "                total += 1\n",
    "    print(f\"✅ 向量化完成：{total} 个片段入库（内存）。\")\n",
    "\n",
    "# Try to ingest if file exists\n",
    "import os\n",
    "if os.path.exists(PDF_PATH):\n",
    "    process_contract_pdf(PDF_PATH)\n",
    "else:\n",
    "    print(\"⚠️ 未找到合同 PDF：\", PDF_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988c7725",
   "metadata": {},
   "source": [
    "\n",
    "## 4️⃣ RAG Retrieval\n",
    "Top-K 相似片段 → 拼接上下文 → 生成 JSON 答案（含引用）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c44b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    denom = (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    if denom == 0:\n",
    "        return 0.0\n",
    "    return float(np.dot(a, b) / denom)\n",
    "\n",
    "def get_similar_chunks(query: str, top_k: int = 5) -> List[Dict[str,Any]]:\n",
    "    if client is None or not VECTOR_DB:\n",
    "        return []\n",
    "    q_emb = client.embeddings.create(\n",
    "        model=\"text-embedding-3-large\",\n",
    "        input=query\n",
    "    ).data[0].embedding\n",
    "    q_emb = np.array(q_emb, dtype=np.float32)\n",
    "    scored = [(cosine_similarity(q_emb, row[\"embedding\"]), row) for row in VECTOR_DB]\n",
    "    scored.sort(key=lambda x: x[0], reverse=True)\n",
    "    return [row for _, row in scored[:top_k]]\n",
    "\n",
    "def build_context(chunks: List[Dict[str,Any]]) -> str:\n",
    "    segs = []\n",
    "    for c in chunks:\n",
    "        snippet = textwrap.shorten(c[\"content\"].strip().replace(\"\\n\",\" \"), width=600, placeholder=\"...\")\n",
    "        segs.append(f\"[Doc {c['doc_id']} p.{c['page']}] {snippet}\")\n",
    "    return \"\\n\\n\".join(segs)\n",
    "\n",
    "def generate_rag_answer(question: str, top_k: int = 5) -> dict:\n",
    "    if client is None:\n",
    "        return {\"answer\":\"(OpenAI not configured)\", \"citations\":[], \"confidence\":\"low\"}\n",
    "    chunks = get_similar_chunks(question, top_k=top_k)\n",
    "    context = build_context(chunks)\n",
    "    prompt = CONTRACT_QA_PROMPT.format(context=context, question=question)\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\":\"user\",\"content\": prompt}],\n",
    "            temperature=0.2\n",
    "        )\n",
    "        raw = resp.choices[0].message.content.strip()\n",
    "        # Try to parse JSON\n",
    "        try:\n",
    "            data = json.loads(raw)\n",
    "        except json.JSONDecodeError:\n",
    "            start, end = raw.find(\"{\"), raw.rfind(\"}\")\n",
    "            if start != -1 and end != -1 and end > start:\n",
    "                try:\n",
    "                    data = json.loads(raw[start:end+1])\n",
    "                except Exception:\n",
    "                    data = {\"answer\": raw, \"citations\": [], \"confidence\": \"medium\"}\n",
    "            else:\n",
    "                data = {\"answer\": raw, \"citations\": [], \"confidence\": \"medium\"}\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        return {\"answer\": f\"Error calling LLM: {e}\", \"citations\": [], \"confidence\": \"low\"}\n",
    "\n",
    "print(\"RAG functions ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17441b2a",
   "metadata": {},
   "source": [
    "\n",
    "## 5️⃣ Unified `generate_reply`\n",
    "按意图分流：合同问答 → RAG；报修/查询 → 返回指引（交给 B/C 表单与API继续处理）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b705c516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_reply(message: str) -> str:\n",
    "    intent = classify_intent(message)\n",
    "    if intent == \"(OpenAI not configured)\":\n",
    "        return intent\n",
    "    if intent == \"contract_qa\":\n",
    "        data = generate_rag_answer(message, top_k=5)\n",
    "        return f\"\"\"{data.get('answer','')}\n",
    "\\n📄 Source: {data.get('citations', [])}\"\"\"\n",
    "    elif intent == \"repair_request\":\n",
    "        return \"🛠️ Repair request noted. Please provide location, issue type, urgency, and photo link.\"\n",
    "    elif intent == \"status_check\":\n",
    "        return \"🔎 Please provide your ticket number (e.g., T2025-001).\"\n",
    "    else:\n",
    "        return \"❓ I’m not sure I understood. Could you rephrase?\"\n",
    "\n",
    "print(\"generate_reply ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33faeab0",
   "metadata": {},
   "source": [
    "\n",
    "## 6️⃣ Quick Tests\n",
    "若未配置 OpenAI 或未导入 PDF，只会返回占位信息；配置后可得到完整答案与引用。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c893ed01",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [\n",
    "    \"When is my rent due?\",\n",
    "    \"What is the amount of security deposit?\",\n",
    "    \"Can I terminate the lease early?\",\n",
    "    \"The toilet is leaking, what should I do?\",\n",
    "]\n",
    "for q in tests:\n",
    "    print(\"Q:\", q)\n",
    "    print(\"A:\", generate_reply(q))\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c38beb",
   "metadata": {},
   "source": [
    "\n",
    "## 7️⃣ Handover Notes (for B/C)\n",
    "- B(DB) provide: `insert_document_chunks(...)` and `get_similar_chunks(...)` for pgvector.\n",
    "- C(API) wrap `generate_reply(message)` under `/ask` and handle auth/roles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26aa134",
   "metadata": {},
   "source": [
    "\n",
    "## ✅ Summary\n",
    "- Prompts, Intent, Ingestion, RAG, Unified reply implemented.\n",
    "- Replace in-memory store with pgvector & connect API for production.\n",
    "\n",
    "Generated at: 2025-10-27 12:16:08\n",
    "\n",
    "\n",
    "**Version:** v3 (Fused prompt integrated at 2025-10-27 12:30:00)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
