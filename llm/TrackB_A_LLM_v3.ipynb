{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d09266ae",
   "metadata": {},
   "source": [
    "\n",
    "# Track B â€” A(LLM) \n",
    "**Scope**: A-role only (LLM, Prompts, RAG, Intent).  \n",
    "**Note**: This notebook uses an in-memory vector store for demo. Replace with pgvector for production.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aff8ad",
   "metadata": {},
   "source": [
    "\n",
    "## 0ï¸âƒ£ Setup\n",
    "- Set `OPENAI_API_KEY` (env var or fill in the cell).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397a979d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal imports and client bootstrap (safe, no hard failure)\n",
    "import os, json, textwrap\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "\n",
    "# Optional imports: guarded to avoid notebook import-time crashes\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "    openai_available = True\n",
    "except Exception:\n",
    "    OpenAI = None\n",
    "    openai_available = False\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "if openai_available and OPENAI_API_KEY:\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "else:\n",
    "    client = None\n",
    "\n",
    "PDF_PATH = \"Track_B_Tenancy_Agreement.pdf\"\n",
    "print(\"OpenAI available:\", openai_available, \"| API key set:\", bool(OPENAI_API_KEY))\n",
    "print(\"PDF path:\", PDF_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe9cdba",
   "metadata": {},
   "source": [
    "\n",
    "## 1ï¸âƒ£ Prompt Templates\n",
    "åˆåŒé—®ç­”ï¼ˆå¸¦å¼•ç”¨ã€JSONè¾“å‡ºï¼‰ä¸æ„å›¾è¯†åˆ«ï¼ˆä¸‰åˆ†ç±»ï¼‰ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94952449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§© Fused Prompt Definitions \n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are an expert assistant for tenancy contracts. \"\n",
    "    \"Use only the information provided in the contract context to answer. \"\n",
    "    \"Cite clause numbers (e.g., 'Clause 2(b)') when referencing them. \"\n",
    "    \"Never fabricate information that is not explicitly supported by the contract. \"\n",
    "    \"If uncertain, say 'Not sure, please check with landlord.' \"\n",
    "    \"Always respond concisely in English.\"\n",
    ")\n",
    "\n",
    "CONTRACT_QA_PROMPT = \"\"\"\n",
    "You are a tenancy contract assistant. \n",
    "Use ONLY the contract excerpts below to answer the user's question.\n",
    "\n",
    "--- CONTRACT CONTEXT START ---\n",
    "{context}\n",
    "--- CONTRACT CONTEXT END ---\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Guidelines:\n",
    "- Answer based strictly on the given clauses.\n",
    "- If the topic is not covered, say: \"Not sure, please check with landlord.\"\n",
    "- Cite clause numbers (e.g., 'Clause 2(b)') where applicable.\n",
    "- Do NOT guess or fabricate.\n",
    "- Provide your final answer in JSON format:\n",
    "{{\n",
    "  \"answer\": \"...\",\n",
    "  \"citations\": [{{\"clause\":\"...\", \"pages\":[...]}}],\n",
    "  \"confidence\": \"high|medium|low\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "print(\"âœ… Fused prompt templates loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37cab76",
   "metadata": {},
   "source": [
    "\n",
    "## 2ï¸âƒ£ Intent Classification\n",
    "æ¸©åº¦ 0ï¼Œè¾“å‡ºä¸¥æ ¼ä¸ºä¸‰é€‰ä¸€ã€‚è‹¥æœªé…ç½® OpenAIï¼Œå°†æç¤ºæœªå°±ç»ªã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8597ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_intent(user_input: str) -> str:\n",
    "    if client is None:\n",
    "        return \"(OpenAI not configured)\"\n",
    "    prompt = INTENT_PROMPT.format(user_input=user_input)\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\":\"user\",\"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    label = resp.choices[0].message.content.strip().lower()\n",
    "    if label not in {\"contract_qa\",\"repair_request\",\"status_check\"}:\n",
    "        label = \"contract_qa\"\n",
    "    return label\n",
    "\n",
    "print(\"Intent function ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7c0737",
   "metadata": {},
   "source": [
    "\n",
    "## 3ï¸âƒ£ Contract Ingestion (PDF â†’ Chunks â†’ Embeddings)\n",
    "ä»¥ token æ•°åˆ†å—å¹¶å‘é‡åŒ–ã€‚æ­¤å¤„ä½¿ç”¨å†…å­˜ `VECTOR_DB` æ¼”ç¤ºï¼›è”è°ƒæ—¶æ›¿æ¢ä¸º **Bç»„** çš„æ•°æ®åº“æ¥å£ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64f2409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer (guarded import)\n",
    "try:\n",
    "    import tiktoken\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    token_ok = True\n",
    "except Exception:\n",
    "    tokenizer = None\n",
    "    token_ok = False\n",
    "\n",
    "VECTOR_DB: List[Dict[str, Any]] = []\n",
    "\n",
    "def split_text_by_tokens(text: str, max_tokens=900, overlap=150):\n",
    "    if not token_ok or tokenizer is None:\n",
    "        # naive fallback by characters if tiktoken not installed\n",
    "        text = (text or \"\").strip()\n",
    "        if not text:\n",
    "            return []\n",
    "        chunks = []\n",
    "        step = 2500  # rough char window\n",
    "        o = 400\n",
    "        for i in range(0, len(text), step - o):\n",
    "            chunks.append(text[i:i+step])\n",
    "        return chunks\n",
    "\n",
    "    text = (text or \"\").strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    toks = tokenizer.encode(text)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(toks):\n",
    "        end = min(start + max_tokens, len(toks))\n",
    "        chunk = tokenizer.decode(toks[start:end])\n",
    "        chunks.append(chunk)\n",
    "        if end == len(toks): break\n",
    "        start = max(0, end - overlap)\n",
    "    return chunks\n",
    "\n",
    "def insert_document_chunk(doc_id: str, page: int, content: str, embedding: List[float]):\n",
    "    VECTOR_DB.append({\n",
    "        \"doc_id\": doc_id,\n",
    "        \"page\": page,\n",
    "        \"content\": content,\n",
    "        \"embedding\": np.array(embedding, dtype=np.float32)\n",
    "    })\n",
    "\n",
    "def process_contract_pdf(pdf_path: str, doc_id: str = \"TA-EXAMPLE\"):\n",
    "    if client is None:\n",
    "        print(\"âš ï¸ OpenAI æœªé…ç½®ï¼Œè·³è¿‡å‘é‡åŒ–ã€‚\")\n",
    "        return\n",
    "    try:\n",
    "        import pdfplumber\n",
    "    except Exception:\n",
    "        print(\"âš ï¸ æœªå®‰è£… pdfplumberï¼Œè·³è¿‡è§£æã€‚\")\n",
    "        return\n",
    "    total = 0\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for i, page in enumerate(pdf.pages, start=1):\n",
    "            text = page.extract_text() or \"\"\n",
    "            for chunk in split_text_by_tokens(text):\n",
    "                emb = client.embeddings.create(\n",
    "                    model=\"text-embedding-3-large\",\n",
    "                    input=chunk\n",
    "                ).data[0].embedding\n",
    "                insert_document_chunk(doc_id, i, chunk, emb)\n",
    "                total += 1\n",
    "    print(f\"âœ… å‘é‡åŒ–å®Œæˆï¼š{total} ä¸ªç‰‡æ®µå…¥åº“ï¼ˆå†…å­˜ï¼‰ã€‚\")\n",
    "\n",
    "# Try to ingest if file exists\n",
    "import os\n",
    "if os.path.exists(PDF_PATH):\n",
    "    process_contract_pdf(PDF_PATH)\n",
    "else:\n",
    "    print(\"âš ï¸ æœªæ‰¾åˆ°åˆåŒ PDFï¼š\", PDF_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988c7725",
   "metadata": {},
   "source": [
    "\n",
    "## 4ï¸âƒ£ RAG Retrieval\n",
    "Top-K ç›¸ä¼¼ç‰‡æ®µ â†’ æ‹¼æ¥ä¸Šä¸‹æ–‡ â†’ ç”Ÿæˆ JSON ç­”æ¡ˆï¼ˆå«å¼•ç”¨ï¼‰ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c44b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    denom = (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    if denom == 0:\n",
    "        return 0.0\n",
    "    return float(np.dot(a, b) / denom)\n",
    "\n",
    "def get_similar_chunks(query: str, top_k: int = 5) -> List[Dict[str,Any]]:\n",
    "    if client is None or not VECTOR_DB:\n",
    "        return []\n",
    "    q_emb = client.embeddings.create(\n",
    "        model=\"text-embedding-3-large\",\n",
    "        input=query\n",
    "    ).data[0].embedding\n",
    "    q_emb = np.array(q_emb, dtype=np.float32)\n",
    "    scored = [(cosine_similarity(q_emb, row[\"embedding\"]), row) for row in VECTOR_DB]\n",
    "    scored.sort(key=lambda x: x[0], reverse=True)\n",
    "    return [row for _, row in scored[:top_k]]\n",
    "\n",
    "def build_context(chunks: List[Dict[str,Any]]) -> str:\n",
    "    segs = []\n",
    "    for c in chunks:\n",
    "        snippet = textwrap.shorten(c[\"content\"].strip().replace(\"\\n\",\" \"), width=600, placeholder=\"...\")\n",
    "        segs.append(f\"[Doc {c['doc_id']} p.{c['page']}] {snippet}\")\n",
    "    return \"\\n\\n\".join(segs)\n",
    "\n",
    "def generate_rag_answer(question: str, top_k: int = 5) -> dict:\n",
    "    if client is None:\n",
    "        return {\"answer\":\"(OpenAI not configured)\", \"citations\":[], \"confidence\":\"low\"}\n",
    "    chunks = get_similar_chunks(question, top_k=top_k)\n",
    "    context = build_context(chunks)\n",
    "    prompt = CONTRACT_QA_PROMPT.format(context=context, question=question)\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\":\"user\",\"content\": prompt}],\n",
    "            temperature=0.2\n",
    "        )\n",
    "        raw = resp.choices[0].message.content.strip()\n",
    "        # Try to parse JSON\n",
    "        try:\n",
    "            data = json.loads(raw)\n",
    "        except json.JSONDecodeError:\n",
    "            start, end = raw.find(\"{\"), raw.rfind(\"}\")\n",
    "            if start != -1 and end != -1 and end > start:\n",
    "                try:\n",
    "                    data = json.loads(raw[start:end+1])\n",
    "                except Exception:\n",
    "                    data = {\"answer\": raw, \"citations\": [], \"confidence\": \"medium\"}\n",
    "            else:\n",
    "                data = {\"answer\": raw, \"citations\": [], \"confidence\": \"medium\"}\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        return {\"answer\": f\"Error calling LLM: {e}\", \"citations\": [], \"confidence\": \"low\"}\n",
    "\n",
    "print(\"RAG functions ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17441b2a",
   "metadata": {},
   "source": [
    "\n",
    "## 5ï¸âƒ£ Unified `generate_reply`\n",
    "æŒ‰æ„å›¾åˆ†æµï¼šåˆåŒé—®ç­” â†’ RAGï¼›æŠ¥ä¿®/æŸ¥è¯¢ â†’ è¿”å›æŒ‡å¼•ï¼ˆäº¤ç»™ B/C è¡¨å•ä¸APIç»§ç»­å¤„ç†ï¼‰ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b705c516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_reply(message: str) -> str:\n",
    "    intent = classify_intent(message)\n",
    "    if intent == \"(OpenAI not configured)\":\n",
    "        return intent\n",
    "    if intent == \"contract_qa\":\n",
    "        data = generate_rag_answer(message, top_k=5)\n",
    "        return f\"\"\"{data.get('answer','')}\n",
    "\\nğŸ“„ Source: {data.get('citations', [])}\"\"\"\n",
    "    elif intent == \"repair_request\":\n",
    "        return \"ğŸ› ï¸ Repair request noted. Please provide location, issue type, urgency, and photo link.\"\n",
    "    elif intent == \"status_check\":\n",
    "        return \"ğŸ” Please provide your ticket number (e.g., T2025-001).\"\n",
    "    else:\n",
    "        return \"â“ Iâ€™m not sure I understood. Could you rephrase?\"\n",
    "\n",
    "print(\"generate_reply ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33faeab0",
   "metadata": {},
   "source": [
    "\n",
    "## 6ï¸âƒ£ Quick Tests\n",
    "è‹¥æœªé…ç½® OpenAI æˆ–æœªå¯¼å…¥ PDFï¼Œåªä¼šè¿”å›å ä½ä¿¡æ¯ï¼›é…ç½®åå¯å¾—åˆ°å®Œæ•´ç­”æ¡ˆä¸å¼•ç”¨ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c893ed01",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [\n",
    "    \"When is my rent due?\",\n",
    "    \"What is the amount of security deposit?\",\n",
    "    \"Can I terminate the lease early?\",\n",
    "    \"The toilet is leaking, what should I do?\",\n",
    "]\n",
    "for q in tests:\n",
    "    print(\"Q:\", q)\n",
    "    print(\"A:\", generate_reply(q))\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c38beb",
   "metadata": {},
   "source": [
    "\n",
    "## 7ï¸âƒ£ Handover Notes (for B/C)\n",
    "- B(DB) provide: `insert_document_chunks(...)` and `get_similar_chunks(...)` for pgvector.\n",
    "- C(API) wrap `generate_reply(message)` under `/ask` and handle auth/roles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26aa134",
   "metadata": {},
   "source": [
    "\n",
    "## âœ… Summary\n",
    "- Prompts, Intent, Ingestion, RAG, Unified reply implemented.\n",
    "- Replace in-memory store with pgvector & connect API for production.\n",
    "\n",
    "Generated at: 2025-10-27 12:16:08\n",
    "\n",
    "\n",
    "**Version:** v3 (Fused prompt integrated at 2025-10-27 12:30:00)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
